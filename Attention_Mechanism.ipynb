{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885b46c9-15fb-41ae-b741-edf8ce68c2c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embed = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bb08f-d7a4-4f14-98e9-501b84dac803",
   "metadata": {},
   "source": [
    "## Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e147a38b-f1d6-4ecf-a80e-cfee002f935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEAD(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, sequence_length, embedding_dim = inputs.shape\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        values = self.value(inputs)\n",
    "        weights = queries @ keys.transpose(-2, -1) * (embedding_dim ** -0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequence_length, :sequence_length]==0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        values = self.value(inputs)\n",
    "        output = weights @ values\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5729e2cd-4355-4961-8687-b012c3f40993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HEAD(\n",
       "  (key): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (query): Linear(in_features=32, out_features=16, bias=False)\n",
       "  (value): Linear(in_features=32, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_head_att = HEAD(16)\n",
    "single_head_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00174f67-cee9-4c4a-9369-88de46cd6042",
   "metadata": {},
   "source": [
    "## Multi Head Attention (Single Head Attention Bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4689a7f1-4147-4bc2-8243-b35cbd0238e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([HEAD(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67d9ea1d-3622-4898-b4c9-3762d4d9b4e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (heads): ModuleList(\n",
       "    (0): HEAD(\n",
       "      (key): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (query): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=16, bias=False)\n",
       "    )\n",
       "    (1): HEAD(\n",
       "      (key): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (query): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=16, bias=False)\n",
       "    )\n",
       "    (2): HEAD(\n",
       "      (key): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (query): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=16, bias=False)\n",
       "    )\n",
       "    (3): HEAD(\n",
       "      (key): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (query): Linear(in_features=32, out_features=16, bias=False)\n",
       "      (value): Linear(in_features=32, out_features=16, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MultiHeadAttention(4, 16)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcea487-b60c-4ca0-a40c-08059fe792be",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "088902a4-0c4c-4608-9d74-47daa04689b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tesnor):\n",
    "        return self.layer(input_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
